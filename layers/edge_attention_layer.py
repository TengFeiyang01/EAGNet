# æ–‡ä»¶è·¯å¾„ï¼šQCNet/layers/edge_attention_layer.py

import torch
import torch.nn as nn
from torch_geometric.utils import softmax

class EdgeAttentionLayer(nn.Module):
    """
    é«˜æ•ˆçš„è¾¹æ³¨æ„åŠ›å±‚ - ä¼˜åŒ–ç‰ˆæœ¬
    ä½¿ç”¨PyTorchå‘é‡åŒ–æ“ä½œï¼Œé¿å…æ‰‹åŠ¨å¾ªç¯
    """

    def __init__(self, hidden_dim: int, edge_dim: int, num_heads: int, dropout: float):
        super(EdgeAttentionLayer, self).__init__()
        self.hidden_dim = hidden_dim
        self.edge_dim = edge_dim
        self.num_heads = num_heads
        self.dropout = dropout

        # å°†èŠ‚ç‚¹ç‰¹å¾æŠ•å½±åˆ°å¤šå¤´æŸ¥è¯¢ (Q)ã€é”® (K)ã€å€¼ (V)
        assert hidden_dim % num_heads == 0, "hidden_dim must be divisible by num_heads"
        self.head_dim = hidden_dim // num_heads

        # çº¿æ€§å˜æ¢çŸ©é˜µï¼šQã€Kã€V
        self.lin_q = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.lin_k = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.lin_v = nn.Linear(hidden_dim, hidden_dim, bias=False)

        # æŠŠè¾¹å±æ€§æ˜ å°„åˆ°æ¯ä¸ª head çš„æƒé‡ç©ºé—´
        self.lin_edge = nn.Linear(edge_dim, num_heads, bias=False)

        self.attn_dropout = nn.Dropout(dropout)
        self.out_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:
        """
        x: èŠ‚ç‚¹ç‰¹å¾ï¼Œå½¢çŠ¶ (N, hidden_dim)
        edge_index: è¾¹ç´¢å¼•ï¼Œå½¢çŠ¶ (2, E)
        edge_attr: è¾¹å±æ€§ï¼Œå½¢çŠ¶ (E, edge_dim)
        è¿”å›ï¼šæ›´æ–°åçš„èŠ‚ç‚¹ç‰¹å¾ (N, hidden_dim)
        """
        N = x.size(0)
        E = edge_index.size(1)

        # æŠ•å½±ä¸ºå¤šå¤´ Qã€Kã€V
        Q = self.lin_q(x).view(N, self.num_heads, self.head_dim)  # (N, num_heads, head_dim)
        K = self.lin_k(x).view(N, self.num_heads, self.head_dim)  # (N, num_heads, head_dim)
        V = self.lin_v(x).view(N, self.num_heads, self.head_dim)  # (N, num_heads, head_dim)

        # edge_index ä¸­ sourceã€target
        src, tgt = edge_index[0], edge_index[1]  # å‡å½¢çŠ¶ (E,)

        # æå–å¯¹åº”è¾¹çš„èŠ‚ç‚¹ç‰¹å¾
        Q_src = Q[src]    # (E, num_heads, head_dim)
        K_tgt = K[tgt]    # (E, num_heads, head_dim)
        V_tgt = V[tgt]    # (E, num_heads, head_dim)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQÂ·K^T / âˆšhead_dim
        attn_score = (Q_src * K_tgt).sum(dim=-1) / (self.head_dim ** 0.5)  # (E, num_heads)

        # è¾¹å±æ€§æ˜ å°„åˆ°æ³¨æ„åŠ›åç½®
        edge_bias = self.lin_edge(edge_attr)  # (E, num_heads)
        attn_score = attn_score + edge_bias

        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨PyTorch Geometricçš„é«˜æ•ˆsoftmax
        # ä¸ºæ¯ä¸ªå¤´åˆ†åˆ«è®¡ç®—softmax
        attn_output = []
        for h in range(self.num_heads):
            # å¯¹æ¯ä¸ªå¤´ï¼Œåœ¨ç›¸åŒtargetèŠ‚ç‚¹ä¸‹åšsoftmax
            attn_h = softmax(attn_score[:, h], tgt, num_nodes=N)  # (E,)
            attn_h = self.attn_dropout(attn_h)
            
            # åŠ æƒèšåˆå€¼
            out_h = attn_h.unsqueeze(-1) * V_tgt[:, h, :]  # (E, head_dim)
            
            # æŒ‰targetèšåˆ
            aggregated_h = torch.zeros(N, self.head_dim, device=x.device, dtype=x.dtype)
            aggregated_h.index_add_(0, tgt, out_h)  # (N, head_dim)
            
            attn_output.append(aggregated_h)

        # æ‹¼æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
        out = torch.cat(attn_output, dim=1)  # (N, hidden_dim)

        # æœ€ç»ˆçº¿æ€§å˜æ¢
        out = self.out_proj(out)  # (N, hidden_dim)
        return out
